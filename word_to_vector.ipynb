{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word To Vector using Gensim\n",
    "\n",
    "## About Gensim\n",
    "Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning. Gensim is implemented in Python and Cython for performance.\n",
    "\n",
    "## What is Word2Vector Embedding?\n",
    "Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim installation and loading the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.26.1)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.11.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m55.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp310-cp310-macosx_12_0_arm64.whl (29.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m567.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m656.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.2 scipy-1.11.3 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m972.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install chardet\n",
    "import chardet\n",
    "with open(\"/Users/aaryasoni/Desktop/DE_assessment/phrases.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))           #pd.read_csv was showing error due to windows encoding in MAC so to reslove this issue i detect the encoding and then specify it in read_csv function below\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                         #importing pandas and numpy\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"phrases.csv\",encoding=\"Windows-1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True, limit=1000000)\n",
    "wv.save_word2vec_format('vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning each word in each phrase a Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000000 300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt; 0.0011291504 -0.00089645386 0.00031852722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in 0.0703125 0.08691406 0.087890625 0.0625 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for -0.011779785 -0.04736328 0.044677734 0.063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that -0.01574707 -0.028320312 0.083496094 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is 0.0070495605 -0.07324219 0.171875 0.0225830...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         1000000 300\n",
       "0  </s> 0.0011291504 -0.00089645386 0.00031852722...\n",
       "1  in 0.0703125 0.08691406 0.087890625 0.0625 0.0...\n",
       "2  for -0.011779785 -0.04736328 0.044677734 0.063...\n",
       "3  that -0.01574707 -0.028320312 0.083496094 0.05...\n",
       "4  is 0.0070495605 -0.07324219 0.171875 0.0225830..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(\"vectors.csv\", on_bad_lines='skip')                 #Feathing word vectors\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1000000 300'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns                                                         #getting name of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=dict()                                                            # saving raw data into a dictionary with word as key and feature vector as value\n",
    "for i in df2['1000000 300']:\n",
    "    s=i.split()\n",
    "    for j in range(1,len(s)):\n",
    "        s[j]=float(s[j])\n",
    "    c[s[0]]=s[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c['is'])                                                        #300 features in a feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cosine distance of each phrase to all other phrases and storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model):                              #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((300,), dtype=\"float32\")                  #inicialize zero vector\n",
    "    nwords = 0                                                      #to find number of words\n",
    "\n",
    "    for word in words:\n",
    "            if(word in model.keys()):\n",
    "                nwords = nwords+1                                   #words increased\n",
    "                featureVec = np.add(featureVec, model[word])        #adding vectors\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)                  #averge vector\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Phrases'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns                                                              #find columns of Phases dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity                      #finding cosine similarity of each phase in the phase.csv with each other\n",
    "c1=dict()                                                                   #dict for storing phase and its cosine similarity\n",
    "for i in df['Phrases']:                                                 \n",
    "    l=[]\n",
    "    for j in df['Phrases']:\n",
    "        if(i==j):\n",
    "            continue\n",
    "        s1=i.split()\n",
    "        s2=j.split()\n",
    "        x=avg_sentence_vector(s1, c)                                        #x is average feature vector of the word i\n",
    "        y=avg_sentence_vector(s2, c)                                        #y is average feature vector of the word y\n",
    "        l.append(1-cosine_similarity(x.reshape(-1, 1),y.reshape(-1, 1)))    #cosine distance is (1-cosine similarity)\n",
    "    l=np.array(l)                                                           \n",
    "    c1[i]=l                                                                 #saving results in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame()                                                          #saving the results in a csv file\n",
    "df3[\"Phase\"]=c1.keys()\n",
    "df3[\"cosine_similarity\"]=c1.values()\n",
    "df3.to_csv(\"cosine_similarity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function that takes any string, e.g. user-input phrase, and finds and return the closest match from phrases in phrases.csv and the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math                                                                #function to find most similar function among phases in phases.csv\n",
    "def find_closest(s):\n",
    "    Max=math.inf\n",
    "    c=0\n",
    "    ans=\"\"\n",
    "    for i in df['Phrases']:\n",
    "        c=wv.wmdistance(s, i)\n",
    "        if(c<Max):\n",
    "            ans=i\n",
    "            Max=c\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What has the capacity movement of airline companies been over the years?\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install POT\n",
    "print(find_closest('how firm differentiates to its employee?'))         #testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(\"w_to_v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2=wv.load(\"w_to_v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
